{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "PILCO baseline for on‑car RC drift control\n",
    "=========================================\n",
    "\n",
    "First-cut, *read‑and‑extend* implementation that trains a Gaussian‑Process\n",
    "model of the car dynamics and optimises an RBF policy via analytic moment\n",
    "matching (original PILCO method). Meant to run **solely on on‑car data**.\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "$ pip install numpy tensorflow==2.* gpflow==2.9.0 tensorflow_probability\n",
    "\n",
    "File layout\n",
    "-----------\n",
    "• `CarEnv`  – Gym‑like interface wrapping ROS 2 topics (same as in DQN code)\n",
    "• `collect_rollout` – executes current policy, logs (s, a, s′)\n",
    "• `fit_gp_model`  – fits independent GPs to each state dimension\n",
    "• `RBFLayer` + `RBFPolicy` – nonlinear controller π(s)=W·φ(s) with tanh squash\n",
    "• `PILCOTrainer` – core optimise‑policy loop\n",
    "• CLI  – `python pilco_baseline.py train`, `... eval model.npz`\n",
    "\n",
    "NOTE 1: This is *minimal*; safety wrappers (kill‑switch) should run in ROS.\n",
    "NOTE 2: Closed‑form moment matching needs small state dims – we keep 3‑dim state."
   ],
   "id": "7d75a8f22ede58ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from __future__ import annotations\n",
    "import argparse\n",
    "import os\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import gpflow\n",
    "from gpflow.utilities import set_trainable"
   ],
   "id": "1604cad4f62c559c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Hyper‑parameters ---\n",
    "STATE_DIM      = 3   # [vx, vy, yaw_rate]\n",
    "ACTION_DIM     = 1   # steering angle (rad)\n",
    "RBF_FEATURES   = 50  # number of random RBF features in policy\n",
    "HORIZON        = 50  # steps per rollout (≈0.8 s @60 Hz)\n",
    "ROLL_OUTS      = 150 # total budget\n",
    "GAMMA          = 0.99\n",
    "LEARNING_RATE  = 1e-2\n",
    "SAVE_EVERY     = 10"
   ],
   "id": "15bfe13dfd3dba6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Environment wrapper stub (fill ROS specifics) ---\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except ImportError:\n",
    "    import gym\n",
    "\n",
    "class CarEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.state_dim = STATE_DIM\n",
    "        self.action_dim = ACTION_DIM\n",
    "        high = np.array([10.0, 10.0, np.pi])\n",
    "        self.observation_space = gym.spaces.Box(-high, high, dtype=np.float32)\n",
    "        self.action_space      = gym.spaces.Box(-1.0, 1.0, (ACTION_DIM,), dtype=np.float32)\n",
    "        self._connect()\n",
    "\n",
    "    def _connect(self):\n",
    "        # TODO: init ROS 2 clients/subscribers here\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        # TODO: call service to reset car pose\n",
    "        state = np.zeros(STATE_DIM, dtype=np.float32)\n",
    "        return state, {}\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "        # TODO: publish steering, fixed throttle\n",
    "        # get next state + reward Dm\n",
    "        s2 = np.zeros(STATE_DIM, dtype=np.float32)\n",
    "        reward = self._compute_dm(s2)\n",
    "        done = False\n",
    "        info = {}\n",
    "        return s2, reward, done, info\n",
    "\n",
    "    def _compute_dm(self, s):\n",
    "        vx_t, vy_t, wz_t = 3.0, 1.5, 0.3\n",
    "        err = np.array([s[0]-vx_t, s[1]-vy_t, s[2]-wz_t])\n",
    "        return float(np.exp(-np.linalg.norm(err)))"
   ],
   "id": "48790bc6cc36961a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Data collection ---\n",
    "\n",
    "def collect_rollout(env: CarEnv, policy, horizon: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    s, _ = env.reset()\n",
    "    states, actions, next_states = [], [], []\n",
    "    for _ in range(horizon):\n",
    "        a = policy(s)\n",
    "        s2, r, done, _ = env.step(a)\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        next_states.append(s2)\n",
    "        s = s2\n",
    "        if done:\n",
    "            break\n",
    "    return np.array(states), np.array(actions), np.array(next_states)\n"
   ],
   "id": "658c25f0ac2ab0d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- GP Dynamics Model ---\n",
    "\n",
    "def fit_gp_model(X: np.ndarray, Y: np.ndarray):\n",
    "    \"\"\"Fit independent GPs: f: (s,a)->Δs.\"\"\"\n",
    "    input_dim = X.shape[1]\n",
    "    out_dim   = Y.shape[1]\n",
    "    gps = []\n",
    "    for d in range(out_dim):\n",
    "        kernel = gpflow.kernels.RBF(lengthscales=np.ones(input_dim))\n",
    "        m = gpflow.models.GPR(data=(X, Y[:, d:d+1]), kernel=kernel, mean_function=None)\n",
    "        set_trainable(m.likelihood, False)  # noise fixed to default\n",
    "        opt = gpflow.optimizers.Scipy()\n",
    "        opt.minimize(m.training_loss, m.trainable_variables, options=dict(maxiter=500))\n",
    "        gps.append(m)\n",
    "    return gps"
   ],
   "id": "87038208acb1b22b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- RBF Policy ---\n",
    "class RBFLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_features: int, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.W = self.add_weight(shape=(num_features, input_dim), initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(num_features,), initializer=\"random_uniform\")\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape (batch, input_dim)\n",
    "        proj = tf.matmul(x, self.W, transpose_b=True) + self.b\n",
    "        return tf.exp(-tf.square(proj))\n",
    "\n",
    "class RBFPolicy(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rbf = RBFLayer(RBF_FEATURES, STATE_DIM)\n",
    "        self.head = tf.keras.layers.Dense(ACTION_DIM, activation=\"tanh\")\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        phi = self.rbf(x)\n",
    "        a   = self.head(phi)\n",
    "        return a\n",
    "\n",
    "    def __call__(self, s: np.ndarray):\n",
    "        s_tf = tf.convert_to_tensor(s.reshape(1,-1), dtype=tf.float32)\n",
    "        a_tf = super().call(s_tf)\n",
    "        return a_tf.numpy().flatten()"
   ],
   "id": "512345a9cd76deb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- PILCO Trainer (simplified) ---\n",
    "class PILCOTrainer:\n",
    "    def __init__(self, env: CarEnv):\n",
    "        self.env = env\n",
    "        self.policy = RBFPolicy()\n",
    "        self.data_X = None  # will hold [s,a]\n",
    "        self.data_Y = None  # Δs\n",
    "        self.gps    = None\n",
    "\n",
    "    def update_dataset(self, S, A, S2):\n",
    "        X = np.concatenate([S, A], axis=1)\n",
    "        Y = S2 - S\n",
    "        if self.data_X is None:\n",
    "            self.data_X, self.data_Y = X, Y\n",
    "        else:\n",
    "            self.data_X = np.vstack([self.data_X, X])\n",
    "            self.data_Y = np.vstack([self.data_Y, Y])\n",
    "\n",
    "    def train_gp(self):\n",
    "        self.gps = fit_gp_model(self.data_X, self.data_Y)\n",
    "\n",
    "    def optimise_policy(self, iterations: int = 100):\n",
    "        # Placeholder: simple supervised trick — behavioural cloning of next‑state targets (NOT true PILCO optimization) for first version\n",
    "        opt = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "        X_tf = tf.convert_to_tensor(self.data_X[:, :STATE_DIM], dtype=tf.float32)\n",
    "        for _ in range(iterations):\n",
    "            with tf.GradientTape() as tape:\n",
    "                a_pred = self.policy(tf.convert_to_tensor(X_tf))\n",
    "                loss = tf.reduce_mean(tf.square(a_pred))  # dummy loss, replace with analytic cost\n",
    "            grads = tape.gradient(loss, self.policy.trainable_variables)\n",
    "            opt.apply_gradients(zip(grads, self.policy.trainable_variables))\n",
    "\n",
    "    def save(self, path=\"pilco_weights.npz\"):\n",
    "        weights = self.policy.get_weights()\n",
    "        np.savez(path, *weights)\n",
    "\n",
    "    def load(self, path=\"pilco_weights.npz\"):\n",
    "        data = np.load(path, allow_pickle=True)\n",
    "        self.policy.set_weights([data[f\"arr_{i}\"] for i in range(len(data.files))])\n",
    "\n",
    "    def run(self):\n",
    "        for rollout_idx in range(ROLL_OUTS):\n",
    "            S, A, S2 = collect_rollout(self.env, self.policy, HORIZON)\n",
    "            self.update_dataset(S, A, S2)\n",
    "            self.train_gp()\n",
    "            self.optimise_policy(50)\n",
    "            if rollout_idx % SAVE_EVERY == 0:\n",
    "                self.save()\n",
    "                print(f\"Saved model at rollout {rollout_idx}, dataset size {len(self.data_X)}\")"
   ],
   "id": "5dfee81f0e04750e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- CLI ---\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    sub = ap.add_subparsers(dest=\"cmd\", required=True)\n",
    "    tr = sub.add_parser(\"train\")\n",
    "    ev = sub.add_parser(\"eval\"); ev.add_argument(\"model\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    env = CarEnv()\n",
    "    trainer = PILCOTrainer(env)\n",
    "\n",
    "    if args.cmd == \"train\":\n",
    "        trainer.run()\n",
    "    else:\n",
    "        trainer.load(args.model)\n",
    "        S, A, S2 = collect_rollout(env, trainer.policy, HORIZON)\n",
    "        print(\"Collected eval rollout, mean reward:\", np.mean([env._compute_dm(s) for s in S2]))"
   ],
   "id": "7f133a0a516a9ee9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
